## Chapter 5 Security Issues

In this chapter, we will discuss security issues that are important in developing applications and systems that use TLS.

--The location of the cause of the vulnerability
--Safety threats and attack methods
--Vulnerability Incident Management

Here's a quick summary of what you should be generally aware of as a developer of applications and systems that use TLS.

### 5.1 Vulnerability hierarchy

The term "vulnerable" does not have a strict definition, but it generally refers to a security issue that causes a loss in the functionality or performance of a software product / system.

However, even if you say "vulnerable" in a word, there are various causes and countermeasures. First, let's look at the factors and properties of each security system hierarchy.

#### 5.1.1 Cryptographic algorithm

The most basic part of security is the individual cryptographic algorithms. The cryptographic algorithms used in security protocols have undergone numerous improvements throughout the history of computer security. The algorithms used today are meticulously designed and standardized, and it is extremely rare for vulnerabilities to be detected in the algorithms themselves at this layer.

However, there are a few things to keep in mind in order to ultimately achieve a secure system using such cryptographic algorithms.

For example, as mentioned in Chapter 3, most cryptographic algorithms today rely on key randomness. Therefore, if the randomness of the key is not guaranteed, the security that is guaranteed by the key length cannot be realized. Also, if the key value can be predicted with a probability higher than the degree of freedom of the key length, it can be said that it is equivalent to using a key that is substantially shorter by that amount, which causes the key value to be easily guessed. It will be.

Advances in digital technology and improvements in performance also benefit the attacker. Even if a cryptographic algorithm with a particular key length is secure at some point in history, it must be considered that over the years, the performance and attack capability will improve along with the attacker's technology. This phenomenon is called "crisis" of the cryptographic algorithm.

As mentioned in Chapters 2 and 3, the types and key lengths of cryptographic algorithms adopted by the protocol standard are being reviewed with the protocol version upgrade. However, reviewing standards often takes a long time, and in many cases the speed of compromise is faster than the evolution of standards. System designers and developers need to consider these issues and appropriately select the algorithm and key length to be used.

Older versions are known to have various vulnerability risks. Also, while the latest TLS 1.2 has many improvements, it hasn't been possible to throw away the old specs for backward compatibility. Therefore, it should be noted that TLS 1.2 involves an unexpected security risk if you do not use it after carefully selecting the cipher suite and various functions to be used (footnote).

### 5.1.2 Protocol specifications

Table: Cryptographic algorithm compromise and protocol specifications

Footnote: For example, NIST SP 800-52 provides guidelines for selecting, configuring, and using TLS implementations.

Older versions are known to have various vulnerability risks. Also, while the latest TLS 1.2 included many improvements, the old specs couldn't be thrown away for backward compatibility. Therefore, when using TLS 1.2, if you do not carefully select the cipher suite to be used and various functions, you will have unexpected security risks.

[Table 5.1 Cryptographic Algorithm Hazard and Protocol Specifications]
| Algorithm type | | Up to TLS1.2 | TLS1.3 |
| --- | --- | --- | --- |
Common key | Block type | DES, TDES, Camellia | AES |
|| Usage Mode | ECB, CBC, CTR, CFB | GCM, CCM, CCM_8 |
|| Stream type | RC4 | Chacha20 |
Hash | MD4, MD5, SHA-1 | SHA-2 / 256, SHA-2 / 384 |

#### 5.1.3 Protocol implementation

The quality / safety of protocol implementation and the response to vulnerabilities will be guaranteed by the efforts of each implementation vendor. For example, wolfSSL, an embedded TLS library vendor, introduces the quality assurance of its products by dividing it into the following items.

##### Basic Quality Assurance

It defines a process from local testing at the developer's hands to validation at Git commit, automated regression testing at pull request, integration testing and peer-reviewed feedback.

##### Automation of quality control

It defines a hybrid CI of onsite and cloud based on Jenkins. This enables automation of quality assurance for embedded hardware architectures.

##### Cryptographic algorithms, modules

Introducing certification by a third-party certification body such as FIPS 140-2 / 3 by NIST.

##### Interoperability test

We are conducting interoperability tests with a large number of other companies' implementations.

##### Unit test

The first unit test is run on the developer's machine, and the test coverage of the unit test is measured weekly and fed back to the developer.

##### API consistency, backward compatibility verification

Implementation changes, enhancements not changing API specifications, and checking consistency between versions are part of daily automated testing.

##### Integration test

We perform tests with various architectures and configurations.

##### Safety analysis

We perform regular analysis with various static analysis tools such as cppcheck, clang static analysis (scan-build), Facebook infer, and valgrind. In addition, the Fuzzing test by our own wolfFjzz validates 4 trillion PRNG seeds in 3 months.

##### Vulnerability management

It defines the process from finding a vulnerability to invoking an action, reviewing the response code without leaking it to an attacker, verifying it, and officially publishing it.

#### 5.1.4 Application implementation

##### TLS version

You should try to use only the latest TLS version as much as possible, but for the time being it is unavoidable to use TLS 1.2. In that case, you should choose the option you want to use to reduce your security risk as much as possible. For example, enable options to avoid potential vulnerability risks, such as enhanced master secrets, and disable risky ones, such as renegotiation.

##### Cipher Suite

TLS 1.3 eliminates all currently risk-aware cipher suites. Therefore, it is possible to use all suites. However, it should be recognized that usage modes CCM, CCM_8, etc. are for relatively low performance MCUs.

##### Random seed

In many TLS implementations, the random numbers actually used in the internal processing are devised to be more random than the random numbers obtained by the random number seed by using pseudo-random number generation or the like. However, care must be taken to increase the degree of freedom of the random number seed value. For example, be aware that simple seed generation with pure software will generate the same random number each time the system boots.

##### Development and testing options

Commercial TLS libraries often provide useful optional features that you can use during application development testing. Taking the above random number seed as an example, we have a random number seed generation function for testing that allows application developers to operate without having to prepare something special at the beginning of development. However, accidentally implementing such an option during operation can lead to unexpected vulnerabilities.

##### Error log

Even after thorough testing and shipping, unexpected abnormalities may occur during operation. In such a case, it is recommended to implement the operation status logging function so that the cause can be surely investigated.

### 5.2 Threats and Attack Techniques

#### 5.2.1 Basic threats on the network

Various security threats are expected in network communication. In the TLS protocol, the main threat is mainly
--Eavesdropping
--Spoofing and unauthorized access
--Falsification
Our goal is to protect information / systems from the three.

##### Eavesdropping

The initial basic purpose of network security was to conceal information flowing over the network. For this purpose, TLS provides a mechanism for encrypting information flowing over the network so that only legitimate recipients can decrypt it. It also includes a mechanism for securely exchanging keys for that purpose. However, that alone is not enough, as shown below.

##### Impersonation, unauthorized access

In TLS usage scenarios, "spoofing" is assumed on both the server and the client.

Taking a web server and a browser as an example, phishing in which a malicious server impersonates another server and steals information from the browser is a typical example of server impersonation. On the other hand, if the client side impersonates another client, there is a risk of illegally accessing the information on the server side or disturbing the operation of the server.

If the client is a small ROM-based device, such as an IoT device or embedded device, the information on the client side may not seem worth the unauthorized access. However, by allowing unauthorized access to the client side and impersonating the client, there is a risk that unauthorized access to the server will be permitted or the operation of the server will be disturbed.

In this way, spoofing and unauthorized access can be performed in the same way as eavesdropping on information that does not appear on the network. In order to prevent spoofing and unauthorized access, it is important to perform peer authentication to confirm that the other party of the communication is a legitimate partner.

##### Tampering

When communicating over a network, there is also the risk of data being tampered with by a man-in-the-middle attack. Even if the recipient successfully decrypts the data, there is no guarantee that it is the original data. So TLS used HMAC-based message authentication on a record-by-record basis to detect tampering.

However, in recent years, it has become clear that this cannot eliminate potential risks, and authenticated encryption (AEAD: Authenticated Encryption with Associated Data), which simultaneously performs data confidentiality and tampering detection, has been adopted. Only the AEAD type is used in the TLS 1.3 cipher suite.

#### 5.2.2 New security threats

In recent years, attackers' motivations for security have become very wide-ranging, from personal interests to financial gains based on large organizations and their collaborations, to national ones.

In the middle of the 2010s, there were cases where systematic, extremely large-scale, long-term leaks came to light, and it was necessary to revise the assumptions of cryptographic and security technologies up to that point. In addition, it is not always possible to expect the operation of IT equipment that requires security in a physically and organizationally robust place such as a data center, or the careful disposal of equipment. Equipment that is inexpensive and widely used in daily life and ordinary business activities is also becoming more and more targeted for security.

For example, in public key cryptography, it is a major premise that the paired private key is securely protected, but it is undeniable that the private key may be leaked for some reason against a large-scale and sophisticated attack. It is also becoming possible to accumulate large amounts of traffic on the network for a very long period of time. In such an environment, even if the communication content is protected at that time, it is possible to go back in time and decrypt a large amount of confidential information when confidential information such as a private key is leaked. It may end up. Against this background, the concept of perfect forward secrecy (PFS), which can guarantee confidentiality even in such a situation, has been proposed and its necessity has come to be recognized.

Initially, TLS used the static RSA method, which was combined with the public key certificate function, as the key exchange method. In this method, the premaster secret generated by the client is encrypted with the RSA public key sent from the server and sent to the server. The RSA public key used at this time is the one stored in the server certificate. With this method, server authentication and key exchange information can be shared, which enables efficient realization. However, on the other hand, it is not practical to renew the server certificate frequently, and you end up reusing the same public key for a long period of time.

In this way, it is difficult to achieve complete forward secrecy with static RSA, so in recent years there has been a shift to the Ephemeral Key method based on Diffie-Hellmann (DH). TLS 1.3 also completely abolishes static RSA and only uses the temporary key Diffie-Hellmann.

#### 5.2.3 Side channel attack

In addition to the complete forward secrecy problem, there are many known attack methods that go beyond the scope of purely algorithmic cryptography. One of them is a series of attack methods called side-channel attacks. Side-channel attacks attempt to read internal information by observing the physical characteristics of the computer performing cryptographic processing from the outside. Specifically, it measures the processing time when performing encryption processing inside the device, changes in power consumption, and physical changes such as electromagnetic waves, sound, and heat generated outside, and uses them as hints for decryption. It is called a side-channel attack because it uses such a "side channel" that is not a gateway for legitimate information.

Normally, hardware-type attacks that do not destroy the attack target are classified as side-channel attacks. It is expected that various side-channel attack methods will continue to emerge, but the following are known so far.

##### Timing attack

This is an attack method that estimates the key information by measuring the difference in processing time due to changing the input value of the device that performs encryption processing.

Among the cryptographic processing, especially in the public key processing, if the principle algorithm is simply realized, the processing time will be significantly different, and it is known that there is a risk that the key can be guessed relatively easily. However, it is known that the processing time can be leveled by carefully implementing it in terms of software, and many cryptographic libraries and the like take such measures.

However, in an environment where detailed timing can be measured in terms of hardware, more accurate measures are required.

##### Breakdown attack

It is an attack that intentionally causes a malfunction or failure due to strong electromagnetic noise from the outside and analyzes the difference from the normal operation.

##### Power analysis attack

It is an attack that infers critical information such as encryption keys by measuring changes in the power consumption / current of the device.

##### Electromagnetic wave analysis attack

Contrary to power analysis, it is an attack that infers critical information by analyzing electromagnetic noise generated from equipment.

##### Cache attack

This is an attack that observes changes in the cache hit rate, etc., depending on the processing content. Note that it may be possible to observe the operating status of cores on the same chip on a multi-core server. To some extent, software measures such as preparing the cache status in advance before processing are possible against this attack.

##### Acoustic analysis attack

It is an attack that guesses the processing content by analyzing the acoustic noise generated from the device.


#### 5.2.4 Hardware layer attack method

Side-channel attack is an attack method that analyzes the operating status of software using electrical characteristics, but if you use a hardware-like method, you can perform a more in-depth analysis. For example, it is possible to read the circuit on the IC chip directly with an optical / laser microscope. In addition to circuit analysis, it is also possible to modify wiring such as disconnection / connection, and attack such as reading data on the bus. Alternatively, it is possible to forcibly branch the operation of the software in an unintended direction by irradiating the IC chip with a laser, read the data, or acquire the information hidden in the software.

Against such hardware layer attacks, countermeasures such as providing a shielded wiring layer to make the circuit wiring invisible, mounting a sensor to detect laser irradiation, and installing a chip destruction detection mechanism are known. Has been done. In addition, the property of withstanding such attacks is called "tamper resistance".

### 5.3 Key management

Digital signatures with public key cryptography allow a third party to verify the validity of the signature without the signing key being known to the outside world (see Section 3.6.5, “Digital Signatures”). This technology can be used to securely store signature keys on a physical hardware unit and provide signature verification capabilities for secure identity management at the hardware level.

A hardware-like device for key management that implements such a key management function physically and securely is called HSM (Hardware Security Management). Traditionally, HSMs have been extremely robust in terms of hardware and have been used as part of large-scale server systems with high tamper resistance. However, in recent years, key management chips called "secure elements" that realize such key management functions on IC chips are also becoming widely used. Such devices are expanding their use, such as to securely manage the identities of lightweight devices such as IoT devices.

With Secure Element, public key pairs are securely managed and sealed during the factory manufacturing process, or the chip is also provided with key generation capabilities to keep all private keys out of the chip throughout the key life cycle. It is designed to be able to perform its function without issuing it. These devices and chips are also typically tamper resistant, ensuring hardware-level safety.

! [Figure 5.1 Principle of key management by secure element] (./fig5-1.png)

Figure 5.1 shows the principle of key management with secure elements. The signing key and verification key are stored in the secure element, and the signing key cannot be referenced from outside the chip. When verifying the signature, obtain the verification key from the corresponding chip in advance and send an appropriate challenge message to the element. The element uses its signature key to generate and return the signature corresponding to the message. The recipient verifies the signature sent using the verification key provided. In this way, the signing key itself can prove that you are the correct signer without any external reference.

The API for HSM access is also being standardized. PKCS # 11 has become a popular early standard as part of RSA's PKCS (see Section 4.2, "PKCS (Public-Key Cryptography Standards)"). PKCS # 11 standardizes APIs for a series of processes, such as key management for relatively large servers and related encryption processes.

On the other hand, as an example of standardization of secure chips and their peripheral services, there is TPM (Trusted Platform Module, ISO / IEC 11889), which is widely used for digital rights management (DRM: Digital Rights Management) and Windows access management. It has been.

These examples include not only simple key management functions, but also various peripheral functions and services. On the other hand, for relatively small devices such as IoT devices, light chips that specialize only in key management are also widely used.

### 5.5 Incident Management

Eradication of vulnerabilities in computer systems is an eternal challenge. In reality, it is important to have a system that can take prompt action when a problem occurs or a potential problem is discovered, and a system that minimizes the impact.

In the world of the Internet, in many cases the number of users is unspecified. Individuals, device and software vendors who discover a vulnerability issue need to ensure that the information is delivered to the users who need it, but it is extremely difficult to deal with it individually. Even for users, it is necessary to always be sure to know the necessary vulnerability information while the many devices and software used are intricately related to each other. Information must be communicated as soon as possible, but uncertain and inaccurate information can have a greater impact and complicate matters.

In the United States, such a need was recognized from an early stage, and NVD (National Vulnerability Database) was put into operation as part of the national defense policy. NVD assigns an ID called CVE (Common Vulnerabilities and Exposures) to each vulnerability incident found and publishes it as a database in an environment that anyone can access.

- https://nvd.nist.gov/

On the other hand, the threat of malware such as computer viruses and worms was also recognized early on. Carnegie Mellon University has set up the CERT / CC (Computer Emergency Response Team Coordination Center) on behalf of the US federal government to collect, analyze, and disclose information (later renamed CSIRT: Computer Security Incident Response Team). ). Currently, National CSIRTs are organized in each country, and JPCERT / CC (JPCERT Coordination Center) is still active in Japan.

On the other hand, the threat of malware such as computer viruses and worms was also recognized early on. Carnegie Mellon University has set up the CERT / CC (Computer Emergency Response Team Coordination Center) on behalf of the US federal government to collect, analyze, and disclose information (later renamed CSIRT: Computer Security Incident Response Team). ). Currently, National CSIRTs are organized in each country, and JPCERT / CC (JPCERT Coordination Center) is still active in Japan.

In addition, JVN (Japan Vulnerability Notes) is operated jointly by IPA (Information-technology Promotion Agency) and JPCERT / CC, and it accepts vulnerability information of product developers in Japan along with NVD information and summarizes the response status. It is open to the public.

- https://jvn.jp/